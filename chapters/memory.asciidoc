[[CH-Memory]]
== The Memory Subsystem: Stacks, Heaps and Garbage Collection

Before we dive into the memory subsystem of ERTS, we need to have some
basic vocabulary and understanding of the general memory layout of a
program in a modern operating system. In this review section I will
assume the program is compiled to an ELF executable and running on
Linux on something like an IA-32/AMD64 architecture. The layout and
terminology is basically the same for all operating systems that ERTS
compile on.

A program's memory layout looks something like this:

[[program_memory_layout]]
.Program Memory Layout
[ditaa]
----
 high
 addresses
        +--------------+
        |   Arguments  |
        |     ENV      |
        +--------------+
        |    Stack     | --+
        |      |       |   | Can grow
        |      v       |   | dynamically
        |              | --+
        +--------------+
        |              | -------------------------------+
        +--------------+                                |
        |    memory    |                                |
        |      map     | -- files or anonymous          |
        |    segment   |                                |
        +--------------+                                |
        |              |                                | Memory
        +--------------+                                | Mapping
        | Thread Stack | --+                            | Region
        |      |       |   | Statically allocated       |
        |      v       |   | on thread start.           |
        |              | --+                            |
        +--------------+                                |
        |              |                                |
        +--------------+                                |
        | Thread Stack | --+                            |
        |      |       |   | Statically allocated       |
        |      v       |   | on thread start.           |
        |              | --+                            |
        +--------------+                                |
        |              | -------------------------------+
        +--------------+ brk
        |              | --+
        |      ^       |   | Can grow
        |      |       |   | dynamically
        |     Heap     | --+
        +--------------+ start_brk
        |     BSS      | --  Static variables initialized to zero
        +--------------+
        |     Data     | --+
        +--------------+   | Binary (disk image)
        |     Code     | --+
        +--------------+
 low
 addresses


----


Even though this picture might look daunting it is still a
simplification. (For a full understanding of the memory subsystem read
a book like "Understanding the Linux Kernel" or "Linux System
Programming") What I want you to take away from this is that there are
two types of dynamically allocatable memory: the heap and memory
mapped segments. I will try to call this heap the _C-heap_ from now
on, to distinguish it from an Erlang process heap. I will call a
memory mapped segment for just a _segment_, and any of the stacks in
this picture for the _C-stack_.

The C-heap is allocated through malloc and a segment is allocated with
mmap.

=== The memory subsystem

Now that we dive into the memory subsystem it will once again
be apparent that ERTS is more like an operating system than just a
programming language environment. Not only does ERTS provide a garbage
collector for Erlang terms on the Erlang process level, but it also
provides a plethora of low level memory allocators and memory
allocation strategies.

For an overview of memory allocators see the erts_alloc documentation
at: http://www.erlang.org/doc/man/erts_alloc.html

All these allocators also comes with a number of parameters that
can be used to tweak their behavior, and this is probably one
of the most important areas from an operational point of view.
This is where we can configure the system behavior to fit anything
from a small embedded control system (like a Raspberry Pi) to an
Internet scale 2TB database server.

There are currently eleven different allocators, six different
allocation strategies, and more than 18 other different settings,
some of which are taking arbitrary numerical values. This
means that there basically is an infinite number of possible
configurations. (OK, strictly speaking it is not infinite, since
each number is bounded, but there are more configurations
than you can shake a stick at.)

In order to be able to use these settings in any meaningful way
we will have to understand how these allocators work and
how each setting impacts the performance of the allocator.

The erts_alloc manual goes as far as to give the following warning:

[quote, Ericsson AB, http://www.erlang.org/doc/man/erts_alloc.html]
____
WARNING: Only use these flags if you are absolutely sure what you are
doing. Unsuitable settings may cause serious performance degradation
and even a system crash at any time during operation.
____

Making you absolutely sure that you know what you are doing, that is
what this chapter is about.

Oh yes, we will also go into details of how the garbage collector
works.


[[SS-Memory_Allocators]]
=== Different type of memory allocators
The Erlang run-time system is trying its best to handle memory
in all situations and under all types of loads, but there are
always corner cases. In this chapter we will look at the details
of how memory is allocated and how the different allocators work.
With this knoweledge and some tools that we will look at later
you should be able to detect and fix problems if your system
ends up in one of these corner cases.

For a nice story about the troubles the system might get into
and how to analyze and correct the behavior read
Fred Hébert’s essay https://blog.heroku.com/archives/2013/11/7/logplex-down-the-rabbit-hole["Troubleshooting Down the Logplex Rabbit Hole"].


When we are talking about a memory allocator in this book we
have a specific meaning in mind. Each memory allocator manage
allocations and deallocations of memory of a certain type.
Each allocator is intended for a specific type of data and is
often specialized for one size of data.

Each memory allocator implements the allocator interface that
can use different algorithms and settings for the actual
memory allocation.

The goal with having different allocators is to reduce
fragmentation, by grouping allocations of the same size,
and to increase performance, by making frequent allocations
cheap.

There are two special, fundamental or generic, memory allocator types
_sys_alloc_ and _mseg_alloc_, and nine specific allocators implemented
through the _alloc_util_ framework.

In the following sections we will go though the different allocators,
with a little detour into the general framework for allocators
(alloc_util).

Each allocator has several names used in the documentation and in the
C code. See xref:table-allocators[] for a short list of all allocators
and their names. The C-name is used in the C-code to refer to the
allocator. The Type-name is used in erl_alloc.types to bind allocation
types to an allocator. The Flag is the letter used for setting
parameters of that allocator when starting Erlang.


.List of memory allocators.
[[table-allocators]]
[options="header"]
|===============================================================================
|Name                    | Description           | C-name     | Type-name | Flag
| Basic allocator        | malloc interface      | sys_alloc  | SYSTEM    | Y
|Memory segment allocator| mmap interface        | mseg_alloc | -         | M
| Temporary allocator    | Temporary allocations | temp_alloc | TEMPORARY | T
| Heap allocator         | Erlang heap data      | eheap_alloc| EHEAP     | H
| Binary allocator       | Binary data           |binary_alloc| BINARY    | B
| ETS allocator          | ETS data              | ets_alloc  | ETS       | E
| Driver allocator       | Driver data           |driver_alloc| DRIVER    | R
| Short lived allocator  | Short lived memory    | sl_alloc   |SHORT_LIVED| S
| Long lived allocator   | Long lived memory     | ll_alloc   |LONG_LIVED | L
| Fixed allocator        | Fixed size data       | fix_alloc  |FIXED_SIZE | F
| Standard allocator     | For most other data   | std_alloc  | STANDARD  | D
|===============================================================================



==== The basic allocator: sys_alloc

The allocator sys_alloc can not be disabled, and is basically a
straight mapping to the underlying OS malloc implementation in
libc.

If a specific allocator is disabled then sys_alloc is used instead.

All specific allocators uses either sys_alloc or mseg_alloc to
allocate memory from the operating system as needed.

When memory is allocated from the OS sys_alloc can add (pad) a fixed
number of kilobytes to the requested number. This can reduce the
number of system calls by over allocating memory. The default padding
is zero.

When memory is freed, sys_alloc will keep some free memory allocated
in the process. The size of this free memory is called the trim
threshold, and the default is 128 kilobytes. This also reduces the
number of system calls at the cost of a higher memory footprint.
This means that if you are running the system with the default
settings you can experience that the Beam process does not give
memory back to the OS directly as memory is freed up.

Memory areas allocated by sys_alloc are stored in the C-heap of the
beam process which will grow as needed through system calls to brk.

==== The memory segment allocator: mseg_alloc

If the underlying operating system supports mmap a specific memory
allocator can use mseg_alloc instead of sys_alloc to allocate
memory from the operating system.

Memory areas allocated through mseg_alloc are called segments. When a
segment is freed it is not immediately returned to the OS, instead it
is kept in a segment cache.

When a new segment is allocated a cached segment is reused if
possible, i.e. if it is the same size or larger than the requested
size but not too large. The value of _absolute max cache bad fit_
determines the number of kilobytes of extra size which is considered
not too large. The default is 4096 kilobytes.

In order not to reuse a 4096 kilobyte segment for really small
allocations there is also a _relative_max_cache_bad_fit_ value which
states that a cached segment may not be used if it is more than
that many percent larger. The default value is 20 percent. That
is a 12 KB segment may be used when asked for a 10 KB segment.

The number of entries in the cache defaults to 10 but can be
set to any value from zero to thirty.

==== The memory allocator framework: alloc_util

Building on top of the two generic allocators (sys_alloc and mseg_alloc)
is a framework called _alloc_util_ which is used to implement specific
memory allocators for different types of usage and data.

The framework is implemented in _erl_alloc_util.[ch]_ and the different
allocators used by ERTS are defined in erl_alloc.types in
the directory "erts/emulator/beam/".

In a SMP system there is usually one allocator of each type per
scheduler thread.

The smallest unit of memory that an allocator can work with is called a
_block_. When you call an allocator to allocate a certain amount of
memory what you get back is a block. It is also blocks that you give
as an argument to the allocator when you want to deallocate memory.

The allocator does not allocate blocks from the operating system
directly though. Instead the allocator allocates a _carrier_ from the
operating system, either through sys_alloc or through mseg_alloc,
which in turn uses malloc or mmap. If sys_alloc is used the carrier
is placed on the C-heap and if mseg_alloc is used the carrier
is placed in a segment.

Small blocks are placed in a multiblock carrier. A multiblock carrier
can as the name suggests contain many blocks. Larger blocks are placed
in a singleblock carrier, which as the name implies on contains one
block.

What's considered a small and a large block is determined by the
parameter _singleblock carrier threshold_ (`sbct`), see the list
of system flags below.

Most allocators also have one "main multiblock carrier" which is never
deallocated.

[ditaa]
----
 high
 addresses
           |FREE OS MEMORY |
           +---------------+ brk
           |   FREE HEAP   |       | less than MYtt kb
           +---------------+
     /     |  Unused PAD   |  | multiple of Muycs
    |      |---------------|  |
    S      |               |  |    |
singleblock|               |  |    |
 carrier 1 |     Block     |  |    | larger than MSsbct kb
    |      |               |  |    |
     \     |               |  |    |
           +---------------+
     /     |Free in Carrier|       |
    |      |---------------|       |
    S      |               |       |
  main     |               |       |
multiblock |     Block 2   |       | MSmmbcs kb
 carrier   |---------------|       |
    |      |               |       |
     \     |     Block 1   |       |
           +---------------+
           |               |
           |    U S E D    |
           |               |
           +---------------+ start_brk
               C-Heap
 low
 addresses


----


===== Blocks, Carriers, and Allocation Strategies

When an Erlang process needs memory, it doesn't directly request it from the operating system with each allocation. Instead, it interacts with specialized allocators provided by the _alloc_util_ framework (implemented in `erl_alloc_util.[ch]`). These allocators handle requests by distributing memory from larger contiguous regions known as "carriers."

Carriers are memory regions allocated directly from the operating system. A carrier is allocated either through:

- `sys_alloc` (using standard C library functions like `malloc()`), placing memory on the process heap, or
- `mseg_alloc` (using `mmap()`), placing memory outside the typical C-heap area.

Carriers are subdivided into smaller memory segments called "blocks." When memory is requested, blocks are allocated from these carriers. There are two primary carrier types:

- **Singleblock Carrier**: A carrier containing exactly one block. Typically used for larger allocations.
- **Multiblock Carrier**: Holds multiple smaller blocks of memory, serving requests for small-to-medium-sized allocations efficiently.

===== Carrier Threshold (`sbct`) and its Impact

The parameter known as _singleblock carrier threshold_ (`sbct`) determines the size boundary between what's considered a "small" and "large" allocation. Allocations larger than the `sbct` value use singleblock carriers, while smaller allocations use multiblock carriers.

===== Why Most Allocations are Preferably Small (Multiblock Carriers)

Multiblock carriers are favored for their efficiency with typical Erlang workloads—many small, short-lived allocations. Research by the HiPE team has shown most Erlang terms are small (less than eight words), fitting neatly into these multiblock carriers.

A typical ERTS allocator usually maintains:

- **One main multiblock carrier** per allocator to handle frequent, small-sized allocations. It rarely releases this carrier, reusing freed blocks internally to reduce fragmentation.
- **Multiple singleblock carriers**, each allocated individually for large objects or binaries. Once these blocks are deallocated, the singleblock carriers are returned to the OS immediately.

====== Memory Layout: Carriers and Blocks (Visualized)

To clarify visually:

[ditaa]
-----------------------------------------

hend ->  +----+
         |....|
stop ->  |    |
         |    |    +----+ old_hend
         |    |    |    |
htop ->  |    |    |    |
         |....|    |    | old_htop
         |....|    |....|
heap ->  +----+    +----+ old_heap
        The Heap   Old Heap

<High Memory Addresses>

+-------------------------------+  OS allocated
|  Singleblock Carrier (> sbct) |
+-------------------------------+
|                               |  
|    Large Allocation (1 block) |
|                               |
|-------------------------------|
|       Unused or padding       |
|-------------------------------|
|          Multiblock           |
|       Carrier (main)          |
|     Block 1 | Block 2 | ...   |
+-------------------------------+
|                               |
|        Allocated Heap         |
|-------------------------------| <- start_brk (OS allocation boundary)
|          C-Heap               |
+-------------------------------+
(low addresses)

-----------------------------------------


===== When to Adjust the `sbct`

Optimizing the singleblock carrier threshold (`sbct`) parameter is a matter of understanding your application's memory allocation patterns. Increasing `sbct` directs more allocations into multiblock carriers, improving memory reuse and reducing fragmentation. This is especially beneficial if your application frequently allocates moderate-sized data structures, causing fragmentation or frequent OS-level memory requests.

Reducing `sbct`, however, forces more allocations into singleblock carriers, making sense when your application occasionally allocates large memory blocks. Managing these large allocations separately simplifies their reclamation and prevents interference with smaller allocations.

If adjusting `sbct` alone does not resolve frequent minor garbage collections—often due to numerous short-lived allocations—consider increasing the process’s initial heap size (`min_heap_size`) to reduce allocation churn.

In practice, the default `sbct` setting is suitable for most Erlang applications. Only fine-tune this parameter if profiling indicates specific problems with fragmentation, memory overhead, or unusual allocation patterns.

===== Memory allocation strategies

To find a free block of memory in a multi block carrier an
allocation strategy is used. Each type of allocator has
a default allocation strategy, but you can also set the
allocation strategy with the `as` flag.

The Erlang Run-Time System Application Reference Manual lists
the following allocation strategies:

[quote,'http://www.erlang.org/doc/man/erts_alloc.html[erts_alloc]']
__________________________

_Best fit_: Find the smallest block that satisfies the requested block size.
(bf)

_Address order best fit_: Find the smallest block that satisfies the
requested block size. If multiple blocks are found, choose the one
with the lowest address.
(aobf)

_Address order first fit_: Find the block with the lowest address that
satisfies the requested block size.
(aoff)

_Address order first fit carrier best fit_ : 
Find the carrier with the lowest address that can satisfy the
requested block size, then find a block within that carrier using the
"best fit" strategy.  (aoffcbf)

_Address order first fit carrier address order best fit_: Find the
carrier with the lowest address that can satisfy the requested block
size, then find a block within that carrier using the "address order
best fit" strategy.
 aoffcaobf (address order first fit carrier address order best fit)


_Good fit_: Try to find the best fit, but settle for the best fit found
during a limited search.
(gf)

_A fit_: Do not search for a fit, inspect only one free block to see if
it satisfies the request. This strategy is only intended to be used
for temporary allocations.
(af)

__________________________





==== The temporary allocator: temp_alloc

The allocator _temp_alloc_, is used for temporary
allocations. That is very short lived allocations. Memory allocated
by temp_alloc may not be allocated over a Erlang process context
switch.

You can use temp_alloc as a small scratch or working area while doing
some work within a function. Look at it as an extension of the C-stack
and free it in the same way. That is, to be on the safe side, free
memory allocated by temp_alloc before returning from the function that
did the allocation. There is a note in erl_alloc.types saying that
you should free a temp_alloc block before the emulator starts
executing Erlang code.

Note that no Erlang process running on the same scheduler as the
allocator may start executing Erlang code before the block is freed.
This means that you can not use a temporary allocation over a BIF
or NIF trap (yield).

In a default R16 SMP system there is N+1 temp_alloc allocators where N
is the number of schedulers. The temp_alloc uses the "A fit" (`af`)
strategy. Since the allocation pattern of the temp_alloc basically is
that of a stack (mostly of size 0 or 1), this strategy works fine.

The temporary allocator is, in R16, used by the following types of
data: TMP_HEAP, MSG_ROOTS, ROOTSET, LOADER_TEMP, NC_TMP, TMP,
DCTRL_BUF, TMP_DIST_BUF, ESTACK, DB_TMP, DB_MC_STK, DB_MS_CMPL_HEAP,
LOGGER_DSBUF, TMP_DSBUF, DDLL_TMP_BUF, TEMP_TERM, SYS_READ_BUF,
ENVIRONMENT, CON_VPRINT_BUF.

For an up to date list of allocation types allocated with each
allocator, see erl_alloc.types
(e.g. `+grep TEMPORARY erts/emulator/beam/erl_alloc.types+`).

I will not go through each of these different types, but in
general as you can guess by their names, they are temporary
buffers or work stacks.


==== The heap allocator: eheap_alloc

The heap allocator, is used for allocating memory blocks
where tagged Erlang terms are stored, such as Erlang process heaps
(all generations), heap fragments, and the beam_registers.

This is probably the memory areas you are most interested in as an
Erlang developer or when tuning an Erlang system. We will talk more
about how these areas are managed in the upcoming sections on garbage
collection and process memory. There we will also cover what a heap
fragment is.


==== The binary allocator: binary_alloc

The binary allocator is used for, yes you guessed it, binaries.
Binaries can be of quite varying sizes and have varying life spans.
This allocator uses the _best fit_ allocation strategy by default.

==== The ETS allocator: ets_alloc

The ETS allocator is used for most ETS related data,
except for some short lived or temporary data used by ETS tables-

==== The driver allocator: driver_alloc

The driver allocator is used for ports, linked in drivers and NIFs.

==== The short lived allocator: sl_alloc

The short lived allocator is used for lists and buffers that are
expected to be short lived. Short lived data can live longer than
temporary data.

// alloc_info_request async bif_timer_sl binary_buffer busy_caller
// busy_caller_table code_ix_lock_q db_fixation db_match_spec_run_heap
// db_proc_cleanup_state ethread_short_lived external_term_data
// extra_port_list extra_root fd_list fixed_del gc_info_request
// misc_aux_work misc_op_list pending_suspend pollset_update_req
// port_names port_task port_task_handle_list prepared_code proc_list
// ptab_list_chunk_info ptab_list_deleted_el ptab_list_pids ptimer_sl
// re_stack re_subject sched_wall_time_request short_lived_thr_queue
// sl_migration_paths ssb system_messages_queue temp_thr_prgr_data
// tmp_cpu_ids unicode_buffer


==== The long lived allocator: ll_alloc

The long lived allocator is used for long lived data, such
as atoms, modules, funs and long lived tables

// atom_entry atom_tab atom_text aux_work_timeouts bif_timer_table code
// code cpu_data cpu_groups_map cs_prog_path db_match_pseudo_proc
// db_match_pseudo_proc db_tabs ddll_errcodes driver_event_state drv_tab
// ethread_long_lived export_entry export_entry export_tab fd_status
// fd_tab fp_exception fun_entry instr_info internal_async_data
// ll_migration_paths ll_temp_term ll_temp_term long_lived_thr_queue
// misc_aux_work_q module_entry module_tab poll_fds poll_result_events
// pollset port_tab pre_alloc_data preloaded proc_lock_waiter proc_tab
// process_interval run_queue_balancing run_queues scheduler_data
// scheduler_data scheduler_sleep_info select_fds taint_list
// thr_prgr_data thr_prgr_internal_data timer_wheel waiter_object



==== The fixed size allocator: fix_alloc

The fixed allocator is used for objects of a fixed size, such as PCBs,
message refs and a few others. The fixed size allocator uses the
_address order best fit_ allocation strategy by default.

// driver_event_data_state driver_select_data_state monitor_sh msg_ref
// nlink_sh proc sl_thr_q_element


==== The standard allocator: std_alloc

The standard allocator is used by the other types of data.
(active_procs alloc_info_request arg_reg bif_timer_ll bits_buf bpd
calls_buf db_heir_data db_heir_data db_named_table_entry dcache
ddll_handle ddll_processes ddll_processes dist_entry dist_tab
driver_lock ethread_standard fd_entry_buf fun_tab gc_info_request
io_queue line_buf link_lh module_refs monitor_lh monitor_lh monitor_sh
nlink_lh nlink_lh nlink_sh node_entry node_tab nodes_monitor
port_data_heap port_lock port_report_exit port_specific_data proc_dict
process_specific_data ptimer_ll re_heap reg_proc reg_tab
sched_wall_time_request stack suspend_monitor thr_q_element thr_queue
zlib )


// === TODO: system flags for memory

// TODO

=== Process Memory

As we saw in xref:CH-Processes[] a process is really just a number
of memory areas, in this chapter we will look a bit closer at how
the stack, the heap and the mailbox are managed.

The default size of the stack and heap is 233 words. This default
size can be changed globally when starting Erlang through the
`pass:[+h]` flag. You can also set the minimum heap size when starting
a process with `spawn_opt` by setting `min_heap_size`.

Erlang terms are tagged as we saw in xref:CH-TypeSystem[], and when
they are stored on the heap they are either cons cells or boxed
objects.


==== Term sharing

Objects on the heap are passed by references within the context of one
process. If you call one function with a tuple as an argument, then
only a tagged reference to that tuple is passed to the called
function. When you build new terms you will also only use references
to sub terms.

For example if you have the string `"hello"` (which is the same as the
list of integers `[104,101,108,108,111]`) you would get a memory layout
similar to:


[[fig-list_layout]]
[ditaa]
----
        ADDR                               BINARY  VALUE + TAG
 hend ->     +-------- -------- -------- --------+
             |                                   |
             |              ...                  |
             |                                   |
             |00000000 00000000 00000000 10000001| 128 + list tag  ----------------+
             |                                   |                                 |
 stop ->     |              ...                  |                                 |
                                                                                   |
                                                                                   |
 htop ->     |              ...                  |                                 |
             |                                   |                                 |
         132 |00000000 00000000 00000000 01111001| 120 + list tag  --------------- | -+
             |                                   |                                 |  |
         128 |00000000 00000000 00000110 10001111| 'h' 104 bsl 4 + small int tag <-+  |
             |                                   |                                    |
         124 |00000000 00000000 00000000 01110001| 112 + list tag  ------------------ | -+
             |                                   |                                    |  |
         120 |00000000 00000000 00000110 01011111| 'e' 101 bsl 4 + small int tag <----+  |
             |                                   |                                       |
         116 |00000000 00000000 00000000 01101001| 104 + list tag  --------------------- | -+
             |                                   |                                       |  |
         112 |00000000 00000000 00000110 11001111| 'l' 108 bsl 4 + small int tag <-------+  |
             |                                   |                                          |
         108 |00000000 00000000 00000000 01100001|  96 + list tag  ------------------------ | -+
             |                                   |                                          |  |
         104 |00000000 00000000 00000110 11001111| 'l' 108 bsl 4 + small int tag <----------+  |
             |                                   |                                             |
         100 |11111111 11111111 11111111 11111011| NIL                                         |
             |                                   |                                             |
          96 |00000000 00000000 00000110 11111111| 'o' 111 bsl 4 + small int tag <-------------+
             |                                   |
             |                ...                |
             |                                   |
 heap ->     +-----------------------------------+

----

If you then create a tuple with two instances of the list, all that is repeated is
the tagged pointer to the list: `00000000000000000000000010000001`. The code

[source,erlang]
----
L = [104, 101, 108, 108, 111],
T = {L, L}.
----

would result in a memory layout as seen below, with T
pointing to a boxed object at address 136, where we find
an ARITYVAL header saying that this is a tuple of size 2 followed by
its two elements, both pointing to the same list L at address 128.

[ditaa]
----
        ADDR                               BINARY  VALUE + TAG
             |              ...                  |
             |                                   |
             |00000000 00000000 00000000 10001010| 136 + boxed tag  ---+
             |                                   |                     |
 stop ->     |              ...                  |                     |
                                                                       |
                                                                       |
 htop ->     |              ...                  |                     |
             |                                   |                     |
         144 |00000000 00000000 00000000 10000001| 128 + list tag  --- | ----------+
             |                                   |                     |           |
         140 |00000000 00000000 00000000 10000001| 128 + list tag  --- | ----------+
             |                                   |                     |           |
         136 |00000000 00000000 00000000 10000000| 2 + ARITYVAL     <--+           |
             |                                   |                                 |
         132 |00000000 00000000 00000000 01111001| 120 + list tag  --------------- | -+
             |                                   |                                 |  |
         128 |00000000 00000000 00000110 10001111| 'h' 104 bsl 4 + small int tag <-+  |
             |                                   |                                    |
             |              ...                  |                                    :

----

This is nice, since it is cheap to do and uses very little space. But if
you send the tuple to another process or do any other type of IO, or any
operations which results in something called a _deep copy_, then the
data structure is expanded. So if we send the tuple `T` to another process
P2 (`pass:[P2 ! T]`) then the heap of T2 will get a tuple where the first
element points to one copy of the string and the second element to another
copy, doubling the amount of space used. You can see the result of this
in the xref:copied_message[section on message passing] further below.

If you have nested shared tuples, this duplication upon deep copying will
grow exponentially with the level of nesting.
You can quickly bring down your Erlang node by expanding a highly shared term,
see <<listing-share,share.erl>>.

[source,erlang]
----
-module(share).

-export([share/2, size/0]).

share(0, Y) -> {Y,Y};
share(N, Y) -> [share(N-1, [N|Y]) || _ <- Y].

size() ->
    T = share:share(5,[a,b,c]),
    {{size, erts_debug:size(T)},
     {flat_size, erts_debug:flat_size(T)}}.



 1> timer:tc(fun() -> share:share(10,[a,b,c]), ok end).
 {1131,ok}

 2> share:share(10,[a,b,c]), ok.
 ok

 3> byte_size(list_to_binary(test:share(10,[a,b,c]))), ok.
 HUGE size (13695500364)
 Abort trap: 6

----

You can calculate the memory size of a shared term and the size of the
expanded size of the term with the functions `erts_debug:size/1` and
`erts_debug:flat_size/1`.

[source,erlang]
----
> share:size().
{{size,19386},{flat_size,94110}}

----

For most applications this is not a problem, but you should be aware
of the problem, which can come up in many situations. A deep copy is
used for IO, ETS tables, binary_to_term, and message passing.

****
It is possible to build ERTS with the configuration option
`--enable-sharing-preserving` which makes the VM discover and preserve
shared terms in these situations, but it is not enabled by default because
it makes sending messages slightly slower in the normal case when there is
no sharing. It has been suggested that this should be the default mode,
since it prevents some very bad situations even if they do not happen
regularly. See xref:AP-BuildingERTS[] for how to build Erlang from source.
****

Let us look in more detail how message passing works.

==== Message passing

When a process P1 sends a message M to another (local) process P2, the
process P1 first calculates the flat size of M. Then it allocates a
new message buffer of that size by doing a heap_alloc of a heap_frag in
the local scheduler context.

Given the code in <<listing-send,send.erl>> the state of the system could
look like this just before the send in p1/1:


[ditaa]
----
         REG
             |                                   |
         x0  |00000000 00000000 00000000 00100011| Pid 2
             |                                   |
         x1  |00000000 00000000 00000000 10001010| 136 + boxed tag  ------+
             |                                   |                        |
                                                                          |
                                                                          |
        ADDR                               BINARY  VALUE + TAG            |
 htop ->     |              ...                  |                        |
             |                                   |                        |
         144 |00000000 00000000 00000000 10000001| 128 + list tag  ------ | -------+
             |                                   |                        |        |
         140 |00000000 00000000 00000000 10000001| 128 + list tag  ------ | -------+
             |                                   |                        |        |
         136 |00000000 00000000 00000000 10000000| 2 + ARITYVAL      <----+        |
             |                                   |                                 |
         132 |00000000 00000000 00000000 01111001| 120 + list tag  --------------- | -+
             |                                   |                                 |  |
         128 |00000000 00000000 00000110 10001111| 'h' 104 bsl 4 + small int tag <-+  |
             |                                   |                                    |
         124 |00000000 00000000 00000000 01110001| 112 + list tag  ------------------ | -+
             |                                   |                                    |  |
         120 |00000000 00000000 00000110 01011111| 'e' 101 bsl 4 + small int tag <----+  |
             |                                   |                                       |
         116 |00000000 00000000 00000000 01101001| 104 + list tag  --------------------- | -+
             |                                   |                                       |  |
         112 |00000000 00000000 00000110 11001111| 'l' 108 bsl 4 + small int tag <-------+  |
             |                                   |                                          |
         108 |00000000 00000000 00000000 01100001|  96 + list tag  ------------------------ | -+
             |                                   |                                          |  |
         104 |00000000 00000000 00000110 11001111| 'l' 108 bsl 4 + small int tag <----------+  |
             |                                   |                                             |
         100 |11111111 11111111 11111111 11111011| NIL                                         |
             |                                   |                                             |
          96 |00000000 00000000 00000110 11111111| 'o' 111 bsl 4 + small int tag <-------------+
             |                                   |
             |                ...                |

----

Then P1 starts sending the message M to P2. The code in
`erl_message.c` first calculates the flat size of M (which in our example is
23 words)footnote:[We ignore tracing here which will add a trace token
to the size of the message, and always use a heap fragment.].
Then (in a SMP system) if it can take a lock on P2 and there is enough
room on the heap of P2 it will copy the message to the heap of P2.

If P2 is running (or exiting) or there isn't enough space on the heap,
then a new heap fragment is allocated
(of sizeof ErlHeapFragment - sizeof(Eterm) + 23*sizeof(Eterm))
footnote:[The -sizeof(Eterm) comes from mem in ErlHeapFragment already
having the size of 1 Eterm] which after initialization will look like:

----
erl_heap_fragment:
    ErlHeapFragment* next;	    NULL
    ErlOffHeap off_heap:
      erl_off_heap_header* first;   NULL
      Uint64 overhead;                 0
    unsigned alloc_size;	      23
    unsigned used_size;               23
    Eterm mem[1];		       ?
      ... 22 free words
----

Then the message is copied into the `mem` part of the heap fragment, and
the `first` pointer is updated (note that memory addresses increase
downwards in this picture, to match the struct layout):

[[copied_message]]
[ditaa]
----
erl_heap_fragment:
           +--------------------+
           |                    |
           |       ...          |
           |                    |
 first ->  |         mem + BOXED| ----+
           |       ...          |     |
           |                    |     |
   mem ->  |          2+ARITYVAL|  <--+
           |                    |
      +1w  |     3w + mem + CONS|  ---+
           |                    |     |
      +2w  |    13w + mem + CONS| --- | --+
           |                    |     |   |
      +3w  |'H' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
      +4w  |     5w + mem + CONS|  ---+   |
           |                    |     |   |
      +5w  |'e' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
      +6w  |     7w + mem + CONS|  ---+   |
           |                    |     |   |
      +7w  |'l' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
      +8w  |     9w + mem + CONS|  ---+   |
           |                    |     |   |
      +9w  |'l' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
     +10w  |    11w + mem + CONS|  ---+   |
           |                    |     |   |
     +11w  |'o' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
     +12w  |                 NIL|         |
           |                    |         |
     +13w  |'H' bsl 4 + SMALLINT|  <------+
           |                    |
     +14w  |    15w + mem + CONS|  ---+
           |                    |     |
     +15w  |'e' bsl 4 + SMALLINT|  <--+
           |                    |
     +16w  |    17w + mem + CONS|  ---+
           |                    |     |
     +17w  |'l' bsl 4 + SMALLINT|  <--+
           |                    |
     +18w  |    19w + mem + CONS|  ---+
           |                    |     |
     +19w  |'l' bsl 4 + SMALLINT|  <--+
           |                    |
     +20w  |    21w + mem + CONS|  ---+
           |                    |     |
     +21w  |'o' bsl 4 + SMALLINT|  <--+
           |                    |
     +22w  |                 NIL|
           |                    |
           +--------------------+

----

In either case a new mbox (`ErlMessage`) is allocated, a lock
 (`ERTS_PROC_LOCK_MSGQ`) is taken on the receiver and the message
 on the heap or in the new heap fragment is linked into the mbox.

[source,c]
----
 erl_mesg {
    struct erl_mesg* next = NULL;
    data:  ErlHeapFragment *heap_frag = bp;
    Eterm m[0]            = message;
 } ErlMessage;

----

Then the mbox is linked into the in message queue (`msg_inq`) of the
receiver, and the lock is released. Note that `msg_inq.last` points to
the `next` field of the last message in the queue. When a new mbox is
linked in this next pointer is updated to point to the new mbox, and
the last pointer is updated to point to the next field of the new
mbox.

[[SS-Binaries]]
==== Binaries

As we saw in xref:CH-TypeSystem[] there are four types of binaries
internally. Three of these types, _heap binaries_, _sub binaries_ and
_match contexts_ are stored on the local heap and handled by the
garbage collector and message passing as any other object, copied as
needed.


===== Reference Counting

The fourth type.  large binaries or _refc binaries_ on the other hand
are partially stored outside of the process heap and they are
reference counted.

The payload of a refc binary is stored in memory allocated by the
binary allocator. There is also a small reference to the payload call
a ProcBin which is stored on the process heap. This reference is
copied by message passing and by the GC, but the payload is
untouched. This makes it relatively cheap to send large binaries to
other processes since the whole binary doesn't need to be copied.

All references through a ProcBin to a refc binary increases the
reference count of the binary by one. All ProcBin objects on a
process heap are linked together in a linked list. After a
GC pass this linked list is traversed and the reference count
of the binary is decreased with one for each ProcBin that
has deceased. If the reference count of the refc binary
reaches zero that binary is deallocated.

Having large binaries reference counted and not copied by send or
garbage collection is a big win, but there is one problem
with having a mixed environment of garbage collection and
reference counting. In a pure reference counted implementation
the reference count would be reduced as soon as a reference to
the object dies, and when the reference count reaches zero the
object is freed. In the ERTS mixed environment a reference to a
reference counted object does not die until a garbage collection
detects that the reference is dead.

This means that binaries, which has a tendency to be large or even
huge, can hang around for a long time after all references to the
binary are dead. Note that since binaries are allocated globally,
all references from all processes need to be dead, that is all
processes that has seen a binary need to do a GC.

Unfortunately it is not always easy, as a developer, to see which
processes have seen a binary in the GC sense of the word seen. Imagine
for example that you have a load balancer that receives work items
and dispatches them to workers.

In <<load_balancer,this code>> there is an example of a loop which
doesn't need to do GC. (See <<listing-lb,listing lb>> for a full example.)

[[load_balancer]]
----
loop(Workers, N) ->
  receive
    WorkItem ->
       Worker = lists:nth(N+1, Workers),
       Worker ! WorkItem,
       loop(Workers, (N+1) rem length(Workers)) 
  end.
----

This server will just keep on grabbing references to binaries and
never free them, eventually using up all system memory.

When one is aware of the problem it is easy to fix, one can either do
a garbage_collect on each iteration of _loop_ or one could do it every
five seconds or so by adding an after clause to the receive. (_after
5000 -> garbage_collect(), loop(Workers, N)_ ).

===== Sub Binaries and Matching

When you match out a part of a binary you get a sub binary.
This sub binary will be a small structure just containing
pointers into the real binary. This increases the reference
count for the binary but uses very little extra space.

If a match would create a new copy of the matched part of the binary
it would cost both space and time. So in most cases just doing a
pattern match on a binary and getting a sub binary to work on is just
what you want.

There are some degenerate cases, imagine for example that you load
huge file like a book into memory and then you match out a small part
like a chapter to work on. The problem is then that the whole of the
rest of the book is still kept in memory until you are done with
processing the chapter. If you do this for many books, perhaps you
want to get the introduction of every book in your file system, then
you will keep the whole of each book in memory and not just the
introductory chapter. This might lead to huge memory usage.

The solution in this case, when you know you only want one small
part of a large binary and you want to have the small part hanging
around for some time, is to use `binary:copy/1`. This function
is only used for its side effect, which is to actually copy
the sub binary out of the real binary removing the reference to
the larger binary and therefore hopefully letting it be garbage
collected.

There is a pretty thorough explanation of how binary construction
and matching is done in the Erlang documentation:
link:http://www.erlang.org/doc/efficiency_guide/binaryhandling.html[].


=== Other interesting memory areas

==== The atom table.

Atoms in Erlang are unique identifiers represented as integers internally. All atoms are stored in a global structure known as the **atom table**. The atom table is a fixed-size structure, meaning there’s an upper limit to how many atoms can exist in a running Erlang system (by default, 1,048,576 atoms). While this might sound like a large number, careless usage (especially dynamically creating atoms from external data) can lead to atom exhaustion, which in turn crashes the entire BEAM VM—an event that's roughly as pleasant as unexpectedly stepping on a LEGO brick in the middle of the night.

Each entry in the atom table contains metadata about an atom, including its string representation (the text of the atom itself), a unique internal identifier used by the runtime system, and additional information like reference counts and details about its usage in modules or functions.

Erlang maintains atoms through three key memory allocator types:

* `atom_text`: Contains the string representations of atoms. This area stores the actual text of atoms.
* `atom_tab`: Stores the atom table itself—a hash table structure for fast lookup.
* `atom_entry`: Allocates memory for each atom's metadata (internal representation, usage counts, etc.).

Atoms are never garbage collected. Once an atom is created, it persists until the VM shuts down. This design decision simplifies implementation (and improves lookup performance), but comes with a risk: an uncontrolled atom creation (commonly through dynamic atom generation via something like `list_to_atom/1`) can lead to exhausting the atom table. Once the atom limit is reached, attempting to create a new atom results in a runtime error, potentially bringing the node down.

Problems often arise when atoms are created carelessly or dynamically, such as when converting user-provided data directly into atoms, parsing large quantities of untrusted external input, or repeatedly generating atoms within loops or recursive functions. These scenarios can lead to rapid atom table growth, potentially exhausting the atom limit and causing severe system issues.

To avoid this:

* Always validate or whitelist user input before converting to atoms.
* Use existing atoms wherever possible or, if dynamic identifiers are required, prefer binaries or strings.
* Monitor the atom table usage regularly using tools like the Observer or built-in functions like 
    - Atom Count (`erlang:system_info(atom_count)`): Number of unique atoms currently loaded.
    - Atom Memory (`erlang:memory(atom)`): Total bytes used by atoms including overhead. 
    - Atom Used Memory (`erlang:memory(atom_used)`): Only the bytes used by the actual atom strings.

A simple check from the Erlang shell can give you a quick indication:

```erlang
1> erlang:system_info(atom_count).
34319
2> erlang:system_info(atom_limit).
1048576
3> erlang:memory(atom).
336049
4> erlang:memory(atom_used).
324520
```

If you ever reach the atom limit, you have two practical solutions:

**Increase the atom table size** (though this is generally a short-term band-aid and should not replace good atom hygiene):

```shell
    erl +t <new_max_atoms>
```

**Redesign your application** to avoid the unlimited creation of atoms—typically by using binaries, strings, or integer identifiers instead.

Thus, atoms and their management require care—misuse can cause stability problems—but when handled correctly, they remain an extremely efficient way of referencing static, known keys or identifiers throughout your system.

To safely convert strings to atoms without risk of atom exhaustion, Erlang provides the `list_to_existing_atom/1` function. This function will only succeed if the atom already exists. If you attempt to create a new atom with this function, it will throw an exception:

[source,erlang]
------------------------------------------

1> list_to_existing_atom("Hello").
** exception error: bad argument
in function  list_to_existing_atom/1
called as list_to_existing_atom("Hello")
*** argument 1: not an already existing atom

2> list_to_existing_atom("true").
true
------------------------------------------


==== Code
Another significant memory area is the code area, where compiled Erlang modules are loaded. Erlang modules, once compiled, are loaded into this code area of memory, which is shared among all processes running within the Erlang runtime system. The code area is generally static and persistent, as modules remain loaded unless explicitly unloaded or replaced (through hot-code loading).

When you load or reload modules using functions such as `l(Module)` or `code:load_file(Module)`, the old code is not immediately removed but kept as the "old" version until no processes reference it. Erlang maintains two versions of each module simultaneously. This allows for safe upgrades without disrupting running processes.

Constants defined in the Erlang code, such as numbers, atoms, and binaries, are stored in a constant pool within the module’s code segment. These constants are efficient in terms of memory usage within a single module, as they are stored only once. However, when constants are used outside of their module—such as during message passing or insertion into ETS tables—they are copied onto the receiving process's heap, potentially increasing overall memory usage significantly.

Monitoring and managing code memory usage is essential, particularly in long-running systems that frequently perform hot-code upgrades. You can inspect loaded modules and their statuses using built-in functions such as `code:all_loaded/0`, and `erlang:memory(code)` to monitor the total memory usage by loaded modules and their constants. 


[source,erlang]
------------------------------------------
1> erlang:memory(code).
6378630
------------------------------------------

